{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Phase B: Mod√®les de Pr√©diction Complets\n",
                "\n",
                "## Bar√®me:\n",
                "- ‚úÖ Repr. N-grammes (1 pt)\n",
                "- ‚úÖ Repr. TF-IDF (1 pt)\n",
                "- ‚úÖ Repr. LLM embeddings (1 pt)\n",
                "- ‚úÖ ML classiques: plusieurs mod√®les (4 pts)\n",
                "- ‚úÖ ML + N-gram (1 pt)\n",
                "- ‚úÖ ML + TF-IDF (1 pt)\n",
                "- ‚úÖ ML + LLM (1 pt)\n",
                "- ‚úÖ Deep Learning: plusieurs architectures (4 pts)\n",
                "- ‚úÖ Deep + N-gram (1 pt)\n",
                "- ‚úÖ Deep + TF-IDF (1 pt)\n",
                "- ‚úÖ Deep + LLM (1 pt)\n",
                "- ‚úÖ Mod√®le inf√©rence optimal (2 pts)\n",
                "- ‚úÖ Mod√®le inf√©rence test (3 pts)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import pickle\n",
                "import os\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.svm import LinearSVC\n",
                "from sklearn.naive_bayes import MultinomialNB\n",
                "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
                "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Create models directory\n",
                "os.makedirs('../models', exist_ok=True)\n",
                "\n",
                "# Load prepared data\n",
                "df = pd.read_parquet(\"../Data/prepared_reviews.parquet\")\n",
                "print(f\"Dataset shape: {df.shape}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Sample for training\n",
                "SAMPLE_SIZE = 30000\n",
                "df_sample = df.sample(min(SAMPLE_SIZE, len(df)), random_state=42)\n",
                "print(f\"Using {len(df_sample)} samples\")\n",
                "\n",
                "# Polarity mapping\n",
                "polarity_map = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
                "df_sample['polarity_num'] = df_sample['polarity'].map(polarity_map)\n",
                "\n",
                "# Split\n",
                "X = df_sample['text']\n",
                "y = df_sample['polarity_num']\n",
                "\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
                "print(f\"Train: {len(X_train)}, Test: {len(X_test)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Repr√©sentation: N-Grammes (CountVectorizer)\n",
                "**+1 pt bar√®me**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# N-Gram Vectorizer (unigrams + bigrams)\n",
                "ngram_vectorizer = CountVectorizer(max_features=5000, ngram_range=(1, 2), stop_words='english')\n",
                "X_train_ngram = ngram_vectorizer.fit_transform(X_train)\n",
                "X_test_ngram = ngram_vectorizer.transform(X_test)\n",
                "print(f\"N-Gram shape: {X_train_ngram.shape}\")\n",
                "\n",
                "# Save vectorizer\n",
                "with open('../models/ngram_vectorizer.pkl', 'wb') as f:\n",
                "    pickle.dump(ngram_vectorizer, f)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Repr√©sentation: TF-IDF\n",
                "**+1 pt bar√®me**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TF-IDF Vectorizer\n",
                "tfidf_vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2), stop_words='english')\n",
                "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
                "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
                "print(f\"TF-IDF shape: {X_train_tfidf.shape}\")\n",
                "\n",
                "# Save vectorizer\n",
                "with open('../models/tfidf_vectorizer.pkl', 'wb') as f:\n",
                "    pickle.dump(tfidf_vectorizer, f)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Repr√©sentation: LLM Embeddings (Sentence Transformers)\n",
                "**+1 pt bar√®me**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sentence_transformers import SentenceTransformer\n",
                "from tqdm import tqdm\n",
                "\n",
                "# Use a smaller, faster model\n",
                "llm_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
                "\n",
                "# Generate embeddings (batch for speed)\n",
                "print(\"Generating LLM embeddings...\")\n",
                "X_train_llm = llm_model.encode(X_train.tolist(), show_progress_bar=True, batch_size=64)\n",
                "X_test_llm = llm_model.encode(X_test.tolist(), show_progress_bar=True, batch_size=64)\n",
                "print(f\"LLM Embeddings shape: {X_train_llm.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# ML CLASSIQUE\n",
                "## 4+ mod√®les = 4 pts"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Store all results\n",
                "results = []\n",
                "\n",
                "def train_and_eval(model, name, X_tr, X_te, y_tr, y_te, representation):\n",
                "    model.fit(X_tr, y_tr)\n",
                "    y_pred = model.predict(X_te)\n",
                "    acc = accuracy_score(y_te, y_pred)\n",
                "    results.append({'Model': name, 'Representation': representation, 'Accuracy': acc})\n",
                "    print(f\"{name} ({representation}): {acc:.4f}\")\n",
                "    return model, y_pred"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4.1 ML + N-Gram (+1 pt)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=== ML + N-Gram ===\")\n",
                "lr_ngram, _ = train_and_eval(LogisticRegression(max_iter=500, n_jobs=-1), 'Logistic Regression', X_train_ngram, X_test_ngram, y_train, y_test, 'N-Gram')\n",
                "svm_ngram, _ = train_and_eval(LinearSVC(max_iter=1000), 'SVM', X_train_ngram, X_test_ngram, y_train, y_test, 'N-Gram')\n",
                "nb_ngram, _ = train_and_eval(MultinomialNB(), 'Naive Bayes', X_train_ngram, X_test_ngram, y_train, y_test, 'N-Gram')\n",
                "rf_ngram, _ = train_and_eval(RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=42), 'Random Forest', X_train_ngram, X_test_ngram, y_train, y_test, 'N-Gram')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4.2 ML + TF-IDF (+1 pt)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\n=== ML + TF-IDF ===\")\n",
                "lr_tfidf, _ = train_and_eval(LogisticRegression(max_iter=500, n_jobs=-1), 'Logistic Regression', X_train_tfidf, X_test_tfidf, y_train, y_test, 'TF-IDF')\n",
                "svm_tfidf, y_pred_best = train_and_eval(LinearSVC(max_iter=1000), 'SVM', X_train_tfidf, X_test_tfidf, y_train, y_test, 'TF-IDF')\n",
                "nb_tfidf, _ = train_and_eval(MultinomialNB(), 'Naive Bayes', X_train_tfidf, X_test_tfidf, y_train, y_test, 'TF-IDF')\n",
                "rf_tfidf, _ = train_and_eval(RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=42), 'Random Forest', X_train_tfidf, X_test_tfidf, y_train, y_test, 'TF-IDF')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4.3 ML + LLM Embeddings (+1 pt)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\n=== ML + LLM Embeddings ===\")\n",
                "lr_llm, _ = train_and_eval(LogisticRegression(max_iter=500, n_jobs=-1), 'Logistic Regression', X_train_llm, X_test_llm, y_train, y_test, 'LLM')\n",
                "svm_llm, _ = train_and_eval(LinearSVC(max_iter=1000), 'SVM', X_train_llm, X_test_llm, y_train, y_test, 'LLM')\n",
                "rf_llm, _ = train_and_eval(RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=42), 'Random Forest', X_train_llm, X_test_llm, y_train, y_test, 'LLM')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# DEEP LEARNING\n",
                "## Plusieurs architectures = 4 pts"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "from torch.utils.data import DataLoader, TensorDataset\n",
                "\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"Using device: {device}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class MLP(nn.Module):\n",
                "    def __init__(self, input_dim, hidden_dim=256, num_classes=3):\n",
                "        super(MLP, self).__init__()\n",
                "        self.model = nn.Sequential(\n",
                "            nn.Linear(input_dim, hidden_dim),\n",
                "            nn.ReLU(),\n",
                "            nn.Dropout(0.3),\n",
                "            nn.Linear(hidden_dim, hidden_dim//2),\n",
                "            nn.ReLU(),\n",
                "            nn.Dropout(0.3),\n",
                "            nn.Linear(hidden_dim//2, num_classes)\n",
                "        )\n",
                "    def forward(self, x):\n",
                "        return self.model(x)\n",
                "\n",
                "class CNN1D(nn.Module):\n",
                "    def __init__(self, input_dim, num_classes=3):\n",
                "        super(CNN1D, self).__init__()\n",
                "        self.conv1 = nn.Conv1d(1, 64, kernel_size=3, padding=1)\n",
                "        self.conv2 = nn.Conv1d(64, 128, kernel_size=3, padding=1)\n",
                "        self.pool = nn.AdaptiveMaxPool1d(1)\n",
                "        self.fc = nn.Linear(128, num_classes)\n",
                "        self.dropout = nn.Dropout(0.3)\n",
                "    def forward(self, x):\n",
                "        x = x.unsqueeze(1)\n",
                "        x = torch.relu(self.conv1(x))\n",
                "        x = torch.relu(self.conv2(x))\n",
                "        x = self.pool(x).squeeze(-1)\n",
                "        x = self.dropout(x)\n",
                "        return self.fc(x)\n",
                "\n",
                "class BiLSTM(nn.Module):\n",
                "    def __init__(self, input_dim, hidden_dim=128, num_classes=3):\n",
                "        super(BiLSTM, self).__init__()\n",
                "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
                "        self.fc = nn.Linear(hidden_dim * 2, num_classes)\n",
                "        self.dropout = nn.Dropout(0.3)\n",
                "    def forward(self, x):\n",
                "        x = x.unsqueeze(1)  # (batch, seq=1, features)\n",
                "        _, (h, _) = self.lstm(x)\n",
                "        h = torch.cat((h[0], h[1]), dim=1)\n",
                "        h = self.dropout(h)\n",
                "        return self.fc(h)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train_dl_model(model, train_loader, epochs=5, lr=0.001):\n",
                "    model.to(device)\n",
                "    criterion = nn.CrossEntropyLoss()\n",
                "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
                "    for epoch in range(epochs):\n",
                "        model.train()\n",
                "        total_loss = 0\n",
                "        for X_batch, y_batch in train_loader:\n",
                "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
                "            optimizer.zero_grad()\n",
                "            outputs = model(X_batch)\n",
                "            loss = criterion(outputs, y_batch)\n",
                "            loss.backward()\n",
                "            optimizer.step()\n",
                "            total_loss += loss.item()\n",
                "        print(f\"  Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader):.4f}\")\n",
                "    return model\n",
                "\n",
                "def eval_dl_model(model, test_loader, y_test_vals, representation, name):\n",
                "    model.eval()\n",
                "    all_preds = []\n",
                "    with torch.no_grad():\n",
                "        for X_batch, _ in test_loader:\n",
                "            X_batch = X_batch.to(device)\n",
                "            outputs = model(X_batch)\n",
                "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
                "            all_preds.extend(preds)\n",
                "    acc = accuracy_score(y_test_vals, all_preds)\n",
                "    results.append({'Model': name, 'Representation': representation, 'Accuracy': acc})\n",
                "    print(f\"{name} ({representation}): {acc:.4f}\")\n",
                "    return all_preds"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5.1 Deep Learning + TF-IDF (+1 pt)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prepare TF-IDF tensors\n",
                "X_train_tensor_tfidf = torch.tensor(X_train_tfidf.toarray(), dtype=torch.float32)\n",
                "X_test_tensor_tfidf = torch.tensor(X_test_tfidf.toarray(), dtype=torch.float32)\n",
                "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
                "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
                "\n",
                "train_ds_tfidf = TensorDataset(X_train_tensor_tfidf, y_train_tensor)\n",
                "test_ds_tfidf = TensorDataset(X_test_tensor_tfidf, y_test_tensor)\n",
                "train_loader_tfidf = DataLoader(train_ds_tfidf, batch_size=128, shuffle=True)\n",
                "test_loader_tfidf = DataLoader(test_ds_tfidf, batch_size=128)\n",
                "\n",
                "print(\"=== Deep Learning + TF-IDF ===\")\n",
                "print(\"Training MLP...\")\n",
                "mlp_tfidf = train_dl_model(MLP(X_train_tfidf.shape[1]), train_loader_tfidf, epochs=5)\n",
                "eval_dl_model(mlp_tfidf, test_loader_tfidf, y_test.values, 'TF-IDF', 'MLP')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5.2 Deep Learning + N-Gram (+1 pt)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prepare N-Gram tensors\n",
                "X_train_tensor_ngram = torch.tensor(X_train_ngram.toarray(), dtype=torch.float32)\n",
                "X_test_tensor_ngram = torch.tensor(X_test_ngram.toarray(), dtype=torch.float32)\n",
                "\n",
                "train_ds_ngram = TensorDataset(X_train_tensor_ngram, y_train_tensor)\n",
                "test_ds_ngram = TensorDataset(X_test_tensor_ngram, y_test_tensor)\n",
                "train_loader_ngram = DataLoader(train_ds_ngram, batch_size=128, shuffle=True)\n",
                "test_loader_ngram = DataLoader(test_ds_ngram, batch_size=128)\n",
                "\n",
                "print(\"=== Deep Learning + N-Gram ===\")\n",
                "print(\"Training CNN...\")\n",
                "cnn_ngram = train_dl_model(CNN1D(X_train_ngram.shape[1]), train_loader_ngram, epochs=5)\n",
                "eval_dl_model(cnn_ngram, test_loader_ngram, y_test.values, 'N-Gram', 'CNN')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5.3 Deep Learning + LLM Embeddings (+1 pt)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prepare LLM tensors\n",
                "X_train_tensor_llm = torch.tensor(X_train_llm, dtype=torch.float32)\n",
                "X_test_tensor_llm = torch.tensor(X_test_llm, dtype=torch.float32)\n",
                "\n",
                "train_ds_llm = TensorDataset(X_train_tensor_llm, y_train_tensor)\n",
                "test_ds_llm = TensorDataset(X_test_tensor_llm, y_test_tensor)\n",
                "train_loader_llm = DataLoader(train_ds_llm, batch_size=128, shuffle=True)\n",
                "test_loader_llm = DataLoader(test_ds_llm, batch_size=128)\n",
                "\n",
                "print(\"=== Deep Learning + LLM ===\")\n",
                "print(\"Training BiLSTM...\")\n",
                "bilstm_llm = train_dl_model(BiLSTM(X_train_llm.shape[1]), train_loader_llm, epochs=5)\n",
                "dl_preds = eval_dl_model(bilstm_llm, test_loader_llm, y_test.values, 'LLM', 'BiLSTM')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 6. Results Summary"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "results_df = pd.DataFrame(results)\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"ALL RESULTS\")\n",
                "print(\"=\"*60)\n",
                "print(results_df.sort_values('Accuracy', ascending=False).to_string(index=False))\n",
                "\n",
                "# Best models\n",
                "best_ml = results_df[~results_df['Model'].isin(['MLP', 'CNN', 'BiLSTM'])].nlargest(1, 'Accuracy')\n",
                "best_dl = results_df[results_df['Model'].isin(['MLP', 'CNN', 'BiLSTM'])].nlargest(1, 'Accuracy')\n",
                "\n",
                "print(f\"\\nüèÜ Best ML Model: {best_ml['Model'].values[0]} ({best_ml['Representation'].values[0]}) - {best_ml['Accuracy'].values[0]:.4f}\")\n",
                "print(f\"üèÜ Best DL Model: {best_dl['Model'].values[0]} ({best_dl['Representation'].values[0]}) - {best_dl['Accuracy'].values[0]:.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualization\n",
                "plt.figure(figsize=(14, 6))\n",
                "sns.barplot(data=results_df, x='Model', y='Accuracy', hue='Representation', palette='Set2')\n",
                "plt.title('Model Comparison by Representation')\n",
                "plt.xticks(rotation=45)\n",
                "plt.ylim(0.5, 1.0)\n",
                "plt.legend(title='Representation')\n",
                "plt.tight_layout()\n",
                "plt.savefig('../references/fig_all_models_comparison.png', dpi=150)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 7. Save Best Models for Inference (+2 pts)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save best ML model (SVM + TF-IDF)\n",
                "with open('../models/best_ml_model.pkl', 'wb') as f:\n",
                "    pickle.dump(svm_tfidf, f)\n",
                "print(\"‚úÖ Best ML model saved: models/best_ml_model.pkl\")\n",
                "\n",
                "# Save best DL model\n",
                "torch.save(mlp_tfidf.state_dict(), '../models/best_dl_model.pth')\n",
                "print(\"‚úÖ Best DL model saved: models/best_dl_model.pth\")\n",
                "\n",
                "# Save model config\n",
                "import json\n",
                "config = {\n",
                "    'ml_model': 'SVM',\n",
                "    'ml_representation': 'TF-IDF',\n",
                "    'dl_model': 'MLP',\n",
                "    'dl_representation': 'TF-IDF',\n",
                "    'tfidf_max_features': 5000,\n",
                "    'input_dim': X_train_tfidf.shape[1],\n",
                "    'polarity_map': {'negative': 0, 'neutral': 1, 'positive': 2},\n",
                "    'inverse_polarity_map': {0: 'negative', 1: 'neutral', 2: 'positive'}\n",
                "}\n",
                "with open('../models/config.json', 'w') as f:\n",
                "    json.dump(config, f, indent=2)\n",
                "print(\"‚úÖ Config saved: models/config.json\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Confusion Matrices"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# ML Confusion Matrix\n",
                "cm_ml = confusion_matrix(y_test, y_pred_best)\n",
                "sns.heatmap(cm_ml, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
                "            xticklabels=['Negative', 'Neutral', 'Positive'],\n",
                "            yticklabels=['Negative', 'Neutral', 'Positive'])\n",
                "axes[0].set_title('ML: SVM + TF-IDF')\n",
                "axes[0].set_xlabel('Predicted')\n",
                "axes[0].set_ylabel('Actual')\n",
                "\n",
                "# DL Confusion Matrix\n",
                "cm_dl = confusion_matrix(y_test, dl_preds)\n",
                "sns.heatmap(cm_dl, annot=True, fmt='d', cmap='Greens', ax=axes[1],\n",
                "            xticklabels=['Negative', 'Neutral', 'Positive'],\n",
                "            yticklabels=['Negative', 'Neutral', 'Positive'])\n",
                "axes[1].set_title('DL: BiLSTM + LLM')\n",
                "axes[1].set_xlabel('Predicted')\n",
                "axes[1].set_ylabel('Actual')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('../references/fig_confusion_matrices.png', dpi=150)\n",
                "plt.show()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}